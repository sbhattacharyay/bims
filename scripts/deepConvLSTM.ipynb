{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import lasagne\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pickle as cp\n",
    "import theano.tensor as T\n",
    "\n",
    "SENSOR_CHANNELS = 42\n",
    "NUM_CLASSES_GCSm = 6\n",
    "NUM_CLASSES_GCSe = 4\n",
    "NUM_CLASSES_GCSv = 5\n",
    "NUM_CLASSES_GCST = 13\n",
    "WINDOW_LENGTH = 721\n",
    "FINAL_SEQUENCE_LENGTH = 705\n",
    "BATCH_SIZE = 32\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZE = 5\n",
    "NUM_UNITS_LSTM = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "input_var = T.tensor4('input')\n",
    "target_var = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sensor Data and GCS Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = os.path.expanduser('~/data/all_motion_feature_data/train_test_sets/')\n",
    "\n",
    "trainArrayGCSe = np.load(os.path.join(baseDir,\"trainArrayGCSe.npy\"))\n",
    "trainArrayGCSm = np.load(os.path.join(baseDir,\"trainArrayGCSm.npy\"))\n",
    "trainArrayGCSv = np.load(os.path.join(baseDir,\"trainArrayGCSv.npy\"))\n",
    "trainArrayGCST = np.load(os.path.join(baseDir,\"trainArrayGCST.npy\"))\n",
    "\n",
    "testArrayGCSe = np.load(os.path.join(baseDir,\"testArrayGCSe.npy\"))\n",
    "testArrayGCSm = np.load(os.path.join(baseDir,\"testArrayGCSm.npy\"))\n",
    "testArrayGCSv = np.load(os.path.join(baseDir,\"testArrayGCSv.npy\"))\n",
    "testArrayGCST = np.load(os.path.join(baseDir,\"testArrayGCST.npy\"))\n",
    "\n",
    "trainLabelsGCSe = np.load(os.path.join(baseDir,\"trainLabelsGCSe.npy\"))\n",
    "trainLabelsGCSm = np.load(os.path.join(baseDir,\"trainLabelsGCSm.npy\"))\n",
    "trainLabelsGCSv = np.load(os.path.join(baseDir,\"trainLabelsGCSv.npy\"))\n",
    "trainLabelsGCST = np.load(os.path.join(baseDir,\"trainLabelsGCST.npy\"))\n",
    "\n",
    "testLabelsGCSe = np.load(os.path.join(baseDir,\"testLabelsGCSe.npy\"))\n",
    "testLabelsGCSm = np.load(os.path.join(baseDir,\"testLabelsGCSm.npy\"))\n",
    "testLabelsGCSv = np.load(os.path.join(baseDir,\"testLabelsGCSv.npy\"))\n",
    "testLabelsGCST = np.load(os.path.join(baseDir,\"testLabelsGCST.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Lasagne Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motor GCS Score Network\n",
    "GCSm_net = {}\n",
    "GCSm_net['input'] = lasagne.layers.InputLayer((BATCH_SIZE, 1, WINDOW_LENGTH, SENSOR_CHANNELS),input_var=input_var)\n",
    "GCSm_net['conv1/5x1'] = lasagne.layers.Conv2DLayer(GCSm_net['input'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSm_net['conv2/5x1'] = lasagne.layers.Conv2DLayer(GCSm_net['conv1/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSm_net['conv3/5x1'] = lasagne.layers.Conv2DLayer(GCSm_net['conv2/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSm_net['conv4/5x1'] = lasagne.layers.Conv2DLayer(GCSm_net['conv3/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSm_net['shuff'] = lasagne.layers.DimshuffleLayer(GCSm_net['conv4/5x1'], (0, 2, 1, 3))\n",
    "GCSm_net['lstm1'] = lasagne.layers.LSTMLayer(GCSm_net['shuff'], NUM_UNITS_LSTM)\n",
    "GCSm_net['lstm2'] = lasagne.layers.LSTMLayer(GCSm_net['lstm1'], NUM_UNITS_LSTM)\n",
    "GCSm_net['shp1'] = lasagne.layers.ReshapeLayer(GCSm_net['lstm2'], (-1, NUM_UNITS_LSTM))\n",
    "GCSm_net['prob'] = lasagne.layers.DenseLayer(GCSm_net['shp1'],NUM_CLASSES_GCSm, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "GCSm_net['shp2'] = lasagne.layers.ReshapeLayer(GCSm_net['prob'], (BATCH_SIZE, FINAL_SEQUENCE_LENGTH, NUM_CLASSES_GCSm))\n",
    "GCSm_net['output'] = lasagne.layers.SliceLayer(GCSm_net['shp2'], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eye GCS Score Network\n",
    "GCSe_net = {}\n",
    "GCSe_net['input'] = lasagne.layers.InputLayer((BATCH_SIZE, 1, WINDOW_LENGTH, SENSOR_CHANNELS),input_var=input_var)\n",
    "GCSe_net['conv1/5x1'] = lasagne.layers.Conv2DLayer(GCSe_net['input'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSe_net['conv2/5x1'] = lasagne.layers.Conv2DLayer(GCSe_net['conv1/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSe_net['conv3/5x1'] = lasagne.layers.Conv2DLayer(GCSe_net['conv2/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSe_net['conv4/5x1'] = lasagne.layers.Conv2DLayer(GCSe_net['conv3/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSe_net['shuff'] = lasagne.layers.DimshuffleLayer(GCSe_net['conv4/5x1'], (0, 2, 1, 3))\n",
    "GCSe_net['lstm1'] = lasagne.layers.LSTMLayer(GCSe_net['shuff'], NUM_UNITS_LSTM)\n",
    "GCSe_net['lstm2'] = lasagne.layers.LSTMLayer(GCSe_net['lstm1'], NUM_UNITS_LSTM)\n",
    "GCSe_net['shp1'] = lasagne.layers.ReshapeLayer(GCSe_net['lstm2'], (-1, NUM_UNITS_LSTM))\n",
    "GCSe_net['prob'] = lasagne.layers.DenseLayer(GCSe_net['shp1'],NUM_CLASSES_GCSe, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "GCSe_net['shp2'] = lasagne.layers.ReshapeLayer(GCSe_net['prob'], (BATCH_SIZE, FINAL_SEQUENCE_LENGTH, NUM_CLASSES_GCSe))\n",
    "GCSe_net['output'] = lasagne.layers.SliceLayer(GCSe_net['shp2'], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbal GCS Score Network\n",
    "GCSv_net = {}\n",
    "GCSv_net['input'] = lasagne.layers.InputLayer((BATCH_SIZE, 1, WINDOW_LENGTH, SENSOR_CHANNELS),input_var=input_var)\n",
    "GCSv_net['conv1/5x1'] = lasagne.layers.Conv2DLayer(GCSv_net['input'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSv_net['conv2/5x1'] = lasagne.layers.Conv2DLayer(GCSv_net['conv1/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSv_net['conv3/5x1'] = lasagne.layers.Conv2DLayer(GCSv_net['conv2/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSv_net['conv4/5x1'] = lasagne.layers.Conv2DLayer(GCSv_net['conv3/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCSv_net['shuff'] = lasagne.layers.DimshuffleLayer(GCSv_net['conv4/5x1'], (0, 2, 1, 3))\n",
    "GCSv_net['lstm1'] = lasagne.layers.LSTMLayer(GCSv_net['shuff'], NUM_UNITS_LSTM)\n",
    "GCSv_net['lstm2'] = lasagne.layers.LSTMLayer(GCSv_net['lstm1'], NUM_UNITS_LSTM)\n",
    "GCSv_net['shp1'] = lasagne.layers.ReshapeLayer(GCSv_net['lstm2'], (-1, NUM_UNITS_LSTM))\n",
    "GCSv_net['prob'] = lasagne.layers.DenseLayer(GCSv_net['shp1'],NUM_CLASSES_GCSv, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "GCSv_net['shp2'] = lasagne.layers.ReshapeLayer(GCSv_net['prob'], (BATCH_SIZE, FINAL_SEQUENCE_LENGTH, NUM_CLASSES_GCSv))\n",
    "GCSv_net['output'] = lasagne.layers.SliceLayer(GCSv_net['shp2'], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total GCS Score Network\n",
    "GCST_net = {}\n",
    "GCST_net['input'] = lasagne.layers.InputLayer((BATCH_SIZE, 1, WINDOW_LENGTH, SENSOR_CHANNELS),input_var=input_var)\n",
    "GCST_net['conv1/5x1'] = lasagne.layers.Conv2DLayer(GCST_net['input'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCST_net['conv2/5x1'] = lasagne.layers.Conv2DLayer(GCST_net['conv1/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCST_net['conv3/5x1'] = lasagne.layers.Conv2DLayer(GCST_net['conv2/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCST_net['conv4/5x1'] = lasagne.layers.Conv2DLayer(GCST_net['conv3/5x1'], NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "GCST_net['shuff'] = lasagne.layers.DimshuffleLayer(GCST_net['conv4/5x1'], (0, 2, 1, 3))\n",
    "GCST_net['lstm1'] = lasagne.layers.LSTMLayer(GCST_net['shuff'], NUM_UNITS_LSTM)\n",
    "GCST_net['lstm2'] = lasagne.layers.LSTMLayer(GCST_net['lstm1'], NUM_UNITS_LSTM)\n",
    "GCST_net['shp1'] = lasagne.layers.ReshapeLayer(GCST_net['lstm2'], (-1, NUM_UNITS_LSTM))\n",
    "GCST_net['prob'] = lasagne.layers.DenseLayer(GCST_net['shp1'],NUM_CLASSES_GCST, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "GCST_net['shp2'] = lasagne.layers.ReshapeLayer(GCST_net['prob'], (BATCH_SIZE, FINAL_SEQUENCE_LENGTH, NUM_CLASSES_GCST))\n",
    "GCST_net['output'] = lasagne.layers.SliceLayer(GCST_net['shp2'], -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-5\n",
    "\n",
    "prediction = lasagne.layers.get_output(GCSm_net['output'])\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "weightsl2 = lasagne.regularization.regularize_network_params(GCSm_net['output'], lasagne.regularization.l2)\n",
    "loss += weight_decay * weightsl2\n",
    "\n",
    "#Get the update rule for Stochastic Gradient Descent with ADAM\n",
    "params = lasagne.layers.get_all_params(GCSm_net['output'], trainable=True)\n",
    "updates = lasagne.updates.adam(loss, params, learning_rate=lr)\n",
    "\n",
    "# Defining training and diagnostics functions:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "test_prediction = lasagne.layers.get_output(GCSm_net['output'], deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc],allow_input_downcast=True)\n",
    "get_preds = theano.function([input_var], test_prediction,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = trainArrayGCSm.shape[0]\n",
    "n_batches = int(np.floor(n_examples / BATCH_SIZE))\n",
    "\n",
    "# Shuffle observations\n",
    "random.seed(2020)\n",
    "p = np.random.permutation(trainArrayGCSm.shape[0])\n",
    "trainArrayGCSm_shuff = trainArrayGCSm[p]\n",
    "trainLabelsGCSm_shuff = trainLabelsGCSm[p] - 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in range(n_batches):\n",
    "        x_batch = trainArrayGCSm_shuff[batch*BATCH_SIZE: (batch+1) * BATCH_SIZE]\n",
    "        y_batch = trainLabelsGCSm_shuff[batch*BATCH_SIZE: (batch+1) * BATCH_SIZE]\n",
    "        train_fn(x_batch, y_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_venv",
   "language": "python",
   "name": "sb_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
